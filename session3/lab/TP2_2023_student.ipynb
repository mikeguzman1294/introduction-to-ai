{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Artificial Intelligence - TP2\n",
    "--\n",
    "\n",
    "At the end of this session, you will be able to : \n",
    "- Perform basic unsupervised learning tasks using sklearn and PyTorch\n",
    "- Apply unsupervised learning on PyRat datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, MiniBatchDictionaryLearning, DictionaryLearning\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits dataset\n",
    "\n",
    "For this TP we are going to be using the DIGITS dataset. The first thing we are going to do is load the dataset. \n",
    "\n",
    "As this is <b>unsupervised</b> we will mostly ignore y (it will only be used for visualization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_digits, y_digits = load_digits(n_class=10, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(3, 10, 3*i+1)\n",
    "    \n",
    "    # pick a random digit in the current category     \n",
    "    curX = x_digits[y_digits==i]    \n",
    "    r = np.random.randint(curX.shape[0])\n",
    "    curim = curX[r, :].reshape((8,8))\n",
    "    \n",
    "    plt.imshow(curim, cmap=plt.cm.gray)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    \n",
    "    plt.subplot(3, 10, 3*i+2)\n",
    "    # pick a random digit in the current category     \n",
    "    curX = x_digits[y_digits==i]    \n",
    "    r = np.random.randint(curX.shape[0])\n",
    "    curim = curX[r, :].reshape((8,8))\n",
    "    \n",
    "    plt.imshow(curim, cmap=plt.cm.gray)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.subplot(3,10,3*i+3)\n",
    "    # pick a random digit in the current category     \n",
    "    curX = x_digits[y_digits==i]    \n",
    "    r = np.random.randint(curX.shape[0])\n",
    "    curim = curX[r, :].reshape((8,8))\n",
    "    \n",
    "    plt.imshow(curim,cmap=plt.cm.gray)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Feature Selection\n",
    "\n",
    "In this part, we will see how to use the [VarianceThreshold](https://scikit-learn.org/stable/modules/feature_selection.html#removing-features-with-low-variance) to remove features with low variance. \n",
    "\n",
    "The principle is to compute variance for each feature, and remove the features that have a variance smaller than a threshold. By default, the method removes only features with a variance of zero (their values are always identical).\n",
    "\n",
    "\n",
    "A word of caution though : the variances are not calculated in the same way whether the set threshold is 0 (the default) or a higher value, as just looking for variances of 0 is done with a method with better numerical precision (see the code [here](https://github.com/scikit-learn/scikit-learn/blob/8c9c1f27b/sklearn/feature_selection/_variance_threshold.py#L115) for a more in depth explanation). \n",
    "\n",
    "\n",
    "In the following, we will use a very small threshold because we will vary the threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "print('original features shape:')\n",
    "print(x_digits.shape)\n",
    "\n",
    "thr = 0.1\n",
    "\n",
    "selector = VarianceThreshold(threshold=thr)\n",
    "x_digits_new=selector.fit_transform(x_digits)\n",
    "\n",
    "print('selected features shape')\n",
    "print(x_digits_new.shape)\n",
    "\n",
    "print(f\"{x_digits.shape[1]-x_digits_new.shape[1]} features were removed\")\n",
    "variances = selector.variances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize which features have been removed ! Generate a boolean mask named `mask` that equals True if a feature must be kept, and False othewise. This can be done by thresholding the variances vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure is organized as follows : \n",
    "- the first row shows original examples of the digits dataset\n",
    "- the second row shows the mask that select features (selected features are in white)\n",
    "- the third row shows the result of masking the original sample from first row with the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots original digits, mask and masked digits \n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    plt.subplot(3, 10, i+1)\n",
    "    # pick a random digit in the current category     \n",
    "    curX = x_digits[y_digits==i]    \n",
    "    r = np.random.randint(curX.shape[0])\n",
    "    curim = curX[r, :].reshape((8,8))\n",
    "    \n",
    "    plt.imshow(curim, cmap=plt.cm.gray)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    #plot mask\n",
    "    plt.subplot(3, 10, i+11)\n",
    "    plt.imshow(mask.reshape((8,8)), cmap=plt.cm.gray)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.subplot(3,10,i+21)\n",
    "    # plot masked image \n",
    "    curim_masked =  curim*mask.reshape((8,8))\n",
    "    \n",
    "    plt.imshow(curim_masked,cmap=plt.cm.gray)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the threshold, you can see more or less features being selected (equivalently, features being removed). \n",
    "\n",
    "In order to know how to select the threshold, you should examine the distribution of feature variances. First, let's visualize it using a histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED : histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may take a decision to choose the threshold directly using this plot, but it will not tell you directly how many features are removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the numpy [percentile](https://numpy.org/doc/stable/reference/generated/numpy.percentile.html) (use the equivalent nanpercentile if you may have NaN in your features) in order to find the threshold that removes a certain percentage of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO BE COMPLETED :\n",
    "## use the numpy percentile function to find the threshold to remove a given percentages of features to keep\n",
    "\n",
    "\n",
    "perc_toremove = 75\n",
    "\n",
    "thresh_perc =   ### TO BE COMPLETED\n",
    "\n",
    "print(f\"A threshold of {thresh_perc} will remove {perc_toremove}% features according to their variance \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to how many % features removed can you still recognize the digits ?  (Qualitative, there is no \"good answer\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part was just to demonstrate a very simple way to remove features in an unsupervised way (you don't need labels). \n",
    "\n",
    "You can also check out [other feature selection methods](https://scikit-learn.org/stable/modules/feature_selection.html) for which you will need the labels. Feel free to experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means Clustering \n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we demonstrate the use of k-means clustering. \n",
    "\n",
    "First, please have a look at the course with notes and read it once again. \n",
    "\n",
    "Done ? We will use the [KMeans estimator from sklearn](https://scikit-learn.org/stable/modules/clustering.html#k-means). Importantly, here are the key concepts :\n",
    "- We define a kmeans estimator like this : `kmeans = KMeans(n_clusters)`. The number of clusters has to be decided in advance, using the n_clusters parameter\n",
    "- The Kmeans clustering is estimated by using the method `fit` on input samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans=KMeans(n_clusters=10, init='k-means++',random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate this clustering on the digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to visualize the centroids of the $10$ clusters. First we have to get the center of each cluster. \n",
    "\n",
    "After having used the method `fit`, the coordinates of the found centroids will be stored in an attribute named `cluster_centers_`, that you can access by doing `kmeans.cluster_centers_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CELL TO COMPLETE, put the cluster centers in variable centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now plot the clusters centroids. Centroids are points in the same space than the original samples, so we can plot them similarly than the samples of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for i,curcen in enumerate(centroids):\n",
    "    \n",
    "    plt.subplot(2, 5, i+1)\n",
    "    im_cen = curcen.reshape((8,8))\n",
    "    plt.imshow(im_cen, cmap=plt.cm.gray)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to take a look in our reconstructions using our KMeans model.\n",
    "\n",
    "First we take a sample from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pick a few random examples \n",
    "whichex = np.random.randint(low=0,high=100,size=1) \n",
    "X_samp = np.concatenate([x_digits[y_digits==i][whichex] for i in range(10)])\n",
    "X_samp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The `transform` method can be used to estimate distances of samples to the centroids\n",
    "- The `predict` method, can be used to assign labels of the closest centroid to each sample\n",
    "\n",
    "Don't hesitate to lookup the help page of these methods using the '?' magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CELL TO COMPLETE, use the transform method from the kmeans object on X_samp.\n",
    "### You will obtain an array containing the distances to the centroids. \n",
    "### Use the argmin method from numpy to generate an array containing the index corresponding to the closest centroid to the samples\n",
    "### Fetch the corresponding centroid in another array closest_centroids.\n",
    "### And finally calculate the distances of each samples to its closest centroid using np.min\n",
    "\n",
    "### TO BE COMPLETED \n",
    "distances = # TO BE COMPLETED (10,10) array containing the distances of each sample to each centroid\n",
    "\n",
    "\n",
    "idx_closest_centroids =  # TO BE COMPLETED (10,) array containing the index closest centroid to the samples \n",
    "closest_centroids= # TO BE COMPLETED (10,64) array containing the closest centroid to each sample \n",
    "smallest_distances =   # TO BE COMPLETED (10,) array containing the distance of each sample to its closest centroid\n",
    "\n",
    "print(distances.shape,idx_closest_centroids.shape,smallest_distances.shape)\n",
    "\n",
    "print(smallest_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### verify that the indices of your closest centroids are the same than the ones obtained using the predict method\n",
    "\n",
    "print(idx_closest_centroids)\n",
    "print(kmeans.predict(X_samp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we plot the samples ans their closest centroids, together with the distance ( = reconstruction error)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i,(im,im_cen,distance) in enumerate(zip(X_samp, closest_centroids, smallest_distances)):\n",
    "        \n",
    "    plt.subplot(4, 6, 1+2*i)\n",
    "    plt.imshow(im.reshape(8, 8), cmap=plt.cm.gray)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Original\")\n",
    "    \n",
    "    plt.subplot(4, 6, 2+2*i)\n",
    "    plt.imshow(im_cen.reshape(8, 8), cmap=plt.cm.gray)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Closest centroid, distance %.2E\"%distance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the elbow method with inertia, by generating KMeans with clusters ranging from 1 to 99, using random state = 0. Inertia is stored in [inertia_](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "inertias = []\n",
    "nclust = range(1,100)\n",
    "### CELL TO COMPLETE, generate a KMeans model with varying n_clusters, fit it to the data \n",
    "### and add its inertia to the inertia list\n",
    "inertias=[]\n",
    "for i in tqdm.tqdm(nclust):\n",
    "\n",
    "    #INSERT CODE HERE\n",
    "    \n",
    "plt.plot(nclust,inertias)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse Dictionary Learning  on Digits\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to pass to dictionary learning. The first thing is to generate a model using [MiniBatchDictionaryLearning](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchDictionaryLearning.html). MiniBatch here means that the method is trained using only a part of the dataset at each time, which helps immensely with the scaling of the method to bigger and wider datasets, but does not guarantee a performance as good as the normal method (where the method see all the dataset at the same time).\n",
    "\n",
    "Use n_components=$16$, random_state=$0$ and verbose = $10$ to instantiate your object. \n",
    "\n",
    "The method [fit_transform](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning.fit_transform) is more useful than fit in this case, as it returns the transformed version generated by the dictionary (denoted U in the course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CELL TO COMPLETE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now retrieve each of the components of the dictionary, and plot them so we can visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = dictlearn.components_\n",
    "fig, axis = plt.subplots(4, 4)\n",
    "for i, d in enumerate(components):\n",
    "    ax = axis[i//4][i%4]\n",
    "    ax.imshow(d.reshape((8, 8)), cmap=plt.cm.gray, vmin=np.min(components), vmax=np.max(components))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the structure of the digits was kept by the components, which means that the digits are always centered and that the rest is a uniform background. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to generate some reconstructions, so first we are going to generate some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichex = np.random.randint(low=0, high=100, size=1) \n",
    "samples = list()\n",
    "indexes = list()\n",
    "for i in range(10):\n",
    "    index = np.where(y_digits==i)[0][whichex]\n",
    "    samples.append(x_digits[index])\n",
    "    indexes.append(index)\n",
    "X_samp = np.concatenate(samples)\n",
    "indexes = np.array(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will use the codes obtained using the dictionary and the indexes used to extract the samples to reconstruct the data by using [np.dot]() function to perform matrix multiplication between the sample code and the components from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CELL TO COMPLETE, generate the reconstructions array using code, indexes and D\n",
    "\n",
    "print(components.shape)\n",
    "print(code.shape)\n",
    "print(x_digits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "for plot_index,(digit,reconstruction) in enumerate(zip(X_samp,reconstructions)):\n",
    "    plt.subplot(2,10,plot_index*2+1)\n",
    "\n",
    "    plt.imshow(digit.reshape((8,8)),cmap=plt.cm.gray,vmin=x_digits.min(),vmax=x_digits.max())\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title('$x$')\n",
    "        \n",
    "    plt.subplot(2,10,plot_index*2+2)\n",
    "    plt.imshow(reconstruction.reshape((8,8)),cmap=plt.cm.gray,vmin=reconstructions.min(),vmax=reconstructions.max())\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    error = np.sum((reconstruction-digit)**2)\n",
    "    plt.title('${\\~x}$, error %.2E' % error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of manifold learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give an example of the importance of manifold learning, we train a TSNE model and plot the 2D visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "unsup = TSNE(random_state=0)\n",
    "examples = unsup.fit_transform(x_digits)\n",
    "plt.scatter(examples[:,0], examples[:,1], c=y_digits)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Unsupervised Learning to PyRat\n",
    "\n",
    "The file features_unsupervised.csv contains a list of features extracted from PyRat games. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas\n",
    "\n",
    "features_unsupervised = pandas.read_csv(\"features_unsupervised.csv\", sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The features are defined as follows : \n",
    "- Distance $d(i,j)$ is the shortest path (minimum number of moves) between two positions $i$ and $j$. In particular, we provide the distance between the rat and the python (distance(rat,python)).\n",
    "- We also provide all distances between the starting position of each player and each cheese.\n",
    "- The number assigned to the cheese for each player are sorted by ascending distances. (distance(rat,cheese_0) is the distance  to the cheese closest to the rat, distance(rat,cheese_1) is the next closest, etc.. same for python )\n",
    "- Density of a cheese $c$ is defined as $density(c) = \\sum_{c' \\neq c}{\\frac{1}{d(c,c')}}$\n",
    "- Density around the starting position of a player $p$ is defined as $density(p) = \\sum_{c}{\\frac{1}{d(p,c)}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the values as a numpy array using `features_unsupervised.values`, as well as all column names using `features_unsupervised.columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_unsupervised.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_unsupervised.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_unsupervised.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first thing we ask you to do is to fit a Kmeans clustering on this data. As we don't provide labels, you'll have to rely on clustering metrics to evaluate the quality of clustering. You can use `inertia_` as we did above, as well as other unsupervised clustering metrics such as the [silhouette coefficent](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient) or the [Calinski Harabasz index](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TO BE COMPLETED\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT 2 - mini challenge \n",
    "\n",
    "Here are instructions for the next project (to be presented next week - **7 minutes + 3 minutes questions**)\n",
    "\n",
    "Main Goal : **Assign cluster labels to each provided sample**\n",
    "\n",
    "During your presentation we will test how well your cluster labels fit with the ground truth labels corresponding to the winner using supervised clustering metric such as the [adjusted rand index](https://scikit-learn.org/stable/modules/clustering.html#rand-index). **Importantly**, you can estimate more than 2 clusters - see the definition of clustering metrics that use ground truth labels, testing for consistency of cluster assignments with ground truth labels. \n",
    "\n",
    "You are free to use combination of unsupervised learning techniques, such as : \n",
    "- Unsupervised feature selection as done above \n",
    "- Manual feature selection and feature engineering (i.e. combining features with a calculation that you define yourself)\n",
    "- [Feature preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- [Decomposition techniques](https://scikit-learn.org/stable/modules/decomposition.html#decompositions) (may be used as a first step before clustering)\n",
    "- [Any clustering algorithm](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods) implemented in sklearn.\n",
    "- [Manifold learning](https://scikit-learn.org/stable/modules/manifold.html#manifold)\n",
    "\n",
    "You can combine any of the techniques mentionned above, feel free to experiment. You can compare your different experiments using the unsupervised metrics we used previouly. \n",
    "\n",
    "In order to evaluate your solution, we ask you to provide us with the estimated cluster labels (numpy array named `labels` of shape `(1000,)`) by saving them with the following command : \n",
    "\n",
    "`np.savez_compressed('labels_binomeX.npz',labels=labels)`\n",
    "\n",
    "And post the file in your binome channel just before the presentations start. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To go further: semi-supervised Learning on PyRat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some situations, data labeling can be pretty expensive, although it is available in large quantity. This can occur when human expertise is required, or when particular resources are needed to determine the label, among other examples. In these cases, one can be interested in what is commonly called *semi-supervised learning*. This paradigm is halfway between supervised and unsupervised learning, and typically consists in attributing labels to unlabeled samples, given a low count of labeled samples of the same classes. Semi-supervised learning can be used to provide a pseudo-label to the unlabeled samples, but also to get better performance than unsupervised learning. Indeed, knowledge of a few labels gives some information that can be exploited.\n",
    "\n",
    "A way to perform semi-supervised clustering is to use a *nearest class mean* (NCM) classifier. Such a classifier computes a barycenter of the labeled samples of a particular class. Then, unsupervised samples are attributed the label of the closest barycenter. Those barycenters can be computed either directly as a mean of the values of all labeled samples per feature, or on a transformed representation of the samples. Such a transformation can be obtained for instance in the middle layer of an autoencoder.\n",
    "\n",
    "Consider the following information:\n",
    "- Rows 20, 140, 257, 394 and 451 in 'features_unsupervised.csv' are of class 0\n",
    "- Rows 29, 258, 369, 500 and 580 in 'features_unsupervised.csv' are of class 1\n",
    "\n",
    "Now program a NCM to give labels to all unlabeled samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_class_mean(train_features, classes, test_features):\n",
    "\n",
    "    \"\"\"Nearest class mean classifier\n",
    "    \n",
    "    Args:\n",
    "        train_features (numpy.ndarray): A matrix of training features (n_samples, n_features)\n",
    "        classes (numpy.array): An array of class labels for each training sample (n_samples,)\n",
    "        test_features (numpy.ndarray): A matrix of test features (n_samples, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: A list of predicted class labels for each test sample (n_samples,)\n",
    "    \"\"\"\n",
    "    \n",
    "    #### TO BE COMPLETED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare information\n",
    "labels_1 = [20, 140, 257, 394, 451]\n",
    "labels_2 = [29, 258, 369, 500, 580]\n",
    "features_unsupervised = features_unsupervised.astype(float)\n",
    "train_samples = features_unsupervised.values[labels_1 + labels_2, :]\n",
    "test_samples = features_unsupervised.values[[i for i in range(features_unsupervised.shape[0]) if i not in labels_1 + labels_2], :]\n",
    "classes = np.array([0] * len(labels_1) + [1] * len(labels_2))\n",
    "\n",
    "# Call the nearest class mean classifier\n",
    "result = nearest_class_mean(train_samples, classes, test_samples)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8be6d65b4a494f6e4f93f1857ec1a6f5288edd1e5f1b656b708bd9b8d81d3faa"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
